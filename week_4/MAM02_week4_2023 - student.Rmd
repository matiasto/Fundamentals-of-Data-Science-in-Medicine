---
title: "MAM02 assignment week 4"
output: html_notebook 
---

SVM and ensemble methods are elective approaches when data set is characterized by a relatively large feature set and by non-linear decision boundaries. In our case, the dataset corresponds to the simulation of 250 proteins measurements of 1,000 individuals (500 cases [cc = 1] and 500 controls [cc = 0]). Data were split into a training set (70% of the whole data, 700 individuals: 350 cases and 350 controls) and a test set (30% of the whole data, 300 individuals: 150 cases and 150 controls). Of the 250 simulated proteins distributions:

-   some were simulated to be weakly associated to the case/control condition (cc),
-   some were simulated to be strongly associated to the case/control condition,
-   others were simulated to be non-associated associated to the case/control condition with no difference in terms of distribution between cases and controls.

# Libraries

```{r}
# install.packages("e1071")
library("pROC")
library("e1071")
library("randomForest")
library("ggplot2")
```

# 1. Import data

```{r}
raw.training <- read.table("simulatedConcentrations.recoded.training.txt", sep = "\t", header = T)
raw.test <- read.table("simulatedConcentrations.recoded.test.txt", sep = "\t", header = T)
raw.training$cc <- as.factor(raw.training$cc)
raw.test$cc <- as.factor(raw.test$cc)
```

## Que 1.1:

**How many rows and columns are in the training and test sets?**

```{r}
ncol(raw.training)
```

Answer: *A: 251

## Que 1.2: 

Compute the correlation matrix for all columns in the training set and plot a histogram of the correlations. Explain in your own words what you can deduce about the data from this. Are there any strong correlations between the variables in the set? Are most correlations positive or negative?

```{r}
# Make a copy of raw.training where the outcome is int instead of factor (needed to include outcome in correlation matrix)
raw.training.outcome_int <- raw.training # copy
raw.training.outcome_int$cc <- as.integer(raw.training.outcome_int$cc) # convert cc to int

# Compute the correlation matrix
# check the cor() function in the help
correlations <- cor(raw.training.outcome_int)
sum(correlations > 0)
sum(correlations <= 0)
# There ae more positive ones overall but by a small margin


# Create histogram via r-base function
# check the hist() function in the help
hist(correlations, breaks = 50)


# Or, create a histogram using ggplot2
# You can use ggplot() + geom_histogram() + theme_linedraw()
# No
```

Answer: There are slighly more positive correalations than negative ones, but this is only marginal. 
The correlations seem to be normally distributed around 0 (based on the histogram)

## Que 1.3:

Using the correlation matrix from the previous question, plot a heatmap of the correlations. \
For ease of checking - try to make its colors range from red (-1) to white (0) to blue (1).\
If you are unfamiliar with the heatmap visualization [here](https://www.jmp.com/en_us/statistics-knowledge-portal/exploratory-data-analysis/heatmap.html) is a short explanation

-   Does the heatmap tell you anything further than the histogram?

-   Where would you look at in the heatmap if you wanted to see correlations between the outcome with the other variables?


```{r fig.height=60, fig.width=60}
# Plot the heatmap
# check the heatmap() function in the help
#breaks
bk <- c(seq(-1,1,by=0.1))
#colors (one less than breaks
mycols <- c(colorRampPalette(colors = c("red","white", "blue"))(length(bk)-1))
heatmap(correlations,breaks=bk, col=mycols, scale="none")

```

Answer: It shows that most of the correlations are indeed randomly distributed, except for a few big correlations. 
In the lower left corner you can see a couple of more correlations, however it is very hard to see which columns are referred to. 
Colors closer to red or blue would indicate higher correlations, so we would look at "bright" spots

## Que 1.4:

**Is it a balanced problem (same number of elements in each class)? Are there any missing values in either train or test sets?**

```{r}
# count elements based on outcome training set
nrow(subset(raw.training, cc == 1))
nrow(subset(raw.training, cc == 0))

nrow(subset(raw.test, cc == 1))
nrow(subset(raw.test, cc == 0))

sum(sapply(raw.training, function(x) sum(is.na(x))) != 0)
sum(sapply(raw.test, function(x) sum(is.na(x))) != 0)
```

Answer: No Missing values. The cc classes are perfectly balanced. 350/350 train-set, 150/150 test-set

# 2. Support vector machines

Fit a linear SVM on the training set data with C classification.

Before we start working on the full 250 variables, just for the fun of if, we created some models with only 2 proteins (to allow visualization in a plot). We selected 2 of the proteins that seem to be more linearly related to the outcome.

First let's plot these 2 proteins to have a visual idea of the dataset, we will use a radial kernel for now...
```{r}

qplot(p_141, p_66, data = raw.training, color = cc )

# Now lets create a model using the different kernels and plot the results to see how the kernels act. We will compare the kernels in a later exercise.

lsvm.fit_only2 <- svm(cc ~ p_141+p_66, data = raw.training, type = "C-classification", kernel = "radial", probability = T)
summary(lsvm.fit_only2)

# performance/AUC
preds = predict(lsvm.fit_only2, newdata=raw.test, probability = TRUE)
pred_probs <- attr(preds, "probabilities")[,2] 
roc <- roc(raw.test$cc, pred_probs)
print(paste("AUC:", auc(roc)))
ci <- ci.auc(raw.test$cc, pred_probs)
print(paste("95% CI:", ci[1], ",", ci[3]))


install.packages("caret")
library(caret)

confusion_table <- table(prediction=preds, actual=raw.test$cc)
confustion_matric <- confusionMatrix(preds, raw.test$cc)                  
accuracy_forFun <- 100*mean(preds==raw.test$cc)
print(paste("The accuracy of the SVM classifier is = ", accuracy_forFun,"%"))

missclassification_forFun <- 100*mean(preds!=raw.test$cc)
print(paste("The missclassifications of the SVM classifier is = ", missclassification_forFun,"%"))
# TODO likely false way and not what they wanted


# Finally lets plot it
plot(lsvm.fit_only2, raw.training, p_141~p_66)

#Lets also take a look at models for the same 2 proteins but using different kernels and see how the AUC is affected
lsvm.fit_only2_linear <- svm(cc ~ p_141+p_66, data = raw.training, type = "C-classification", kernel = "linear", probability = T)
lsvm.fit_only2_poly <- svm(cc ~ p_141+p_66, data = raw.training, type = "C-classification", kernel = "polynomial", probability = T)
lsvm.fit_only2_radial <- svm(cc ~ p_141+p_66, data = raw.training, type = "C-classification", kernel = "radial", probability = T)
lsvm.fit_only2_sigmoid <- svm(cc ~ p_141+p_66, data = raw.training, type = "C-classification", kernel = "sigmoid", probability = T)
plot(lsvm.fit_only2_linear, raw.training, p_141~p_66)
plot(lsvm.fit_only2_poly, raw.training, p_141~p_66)
plot(lsvm.fit_only2_radial, raw.training, p_141~p_66)
plot(lsvm.fit_only2_sigmoid, raw.training, p_141~p_66)

# And their AUC`s
get_auc <- function(model) {
  preds = predict(model, newdata=raw.test, probability = TRUE)
  pred_probs <- attr(preds, "probabilities")[,2] 
  roc <- roc(raw.test$cc, pred_probs)
  plot(roc)
  auc <- auc(roc)
  return(auc)
}
print(get_auc(lsvm.fit_only2_linear))
print(get_auc(lsvm.fit_only2_poly))
print(get_auc(lsvm.fit_only2_radial))
print(get_auc(lsvm.fit_only2_sigmoid))

```
We performed the steps above even though they are not part of the assignment to be able to have a better understanding of the SVM model and of the dataset we are working with (with visualization)...

Now lets stop playing around with only 2 dimensions... back to the assignment, lets fit the SVM using 250 variables (there are ways to visualize 5 dimensions in a plot (using color and size for example), or 6 dimensions (adding frequency of vibration) if animation is used... for 250 dimensions it is impractical)
```{r}
lsvm.fit <- svm(cc ~ ., data = raw.training, type = "C-classification", kernel = "linear", probability = T)
```

## Que 2.1

Check the `svm` API in the help to help you answer this. **What is the meaning of setting `probability = T`? What does change if you set it to false? And if you remove it, what is the default? ** Look at the documentation and also try setting it differently in your code and check what changes.

```{r}
?svm
```

Answer: The setting probability = T allows for probability predictions

## Que 2.2

Inspect the resulting support vectors (number and type). Check the `svm` API in the help and think how you can achieve this. **What is the percentage of training data used for classification? Which support vector has the largest magnitude? Which has the smallest?**

```{r}
View(lsvm.fit$SV)
sapply(lsvm.fit$SV, class)
nrow(lsvm.fit$SV)
ncol(lsvm.fit$SV)
```

Answer: there are 250 support Vecors with a length of 216. All vectors are numeric. 
TODO  ?

## Que 2.3

Predict the class of the test set and estimate the discriminative performances by computing the AUC. **What is the AUC? Is this bad, acceptable, good, ...? Optional: What is the 95% CI of the AUC? what does it mean?**

```{r}
preds <- predict(lsvm.fit, raw.test, probability = TRUE)
pred_probs <- attr(preds, "probabilities")[,2] 
roc <- roc(raw.test$cc, pred_probs)
plot(roc, main="ROC AUC or the lsvm model")
print(paste("AUC:", auc(roc)))
ci <- ci.auc(raw.test$cc, pred_probs)
print(paste("95% CI:", ci[1], ",", ci[3]))

```

Answer: The ROC is pretty high for the dataset with a AUC or ~0.89 and a confidence interval form ~0.86 to ~0.93. This means that the true AUC is between 0.86 and 0.93. Both are acceptably high values

## Que 2.4

Repeat the exercise by varying the value of the parameter "cost" and generate a table of results with the value of cost and the corresponding number of support vectors, and AUC on the test data. Use at least these 3 costs: [0.1, 1, 10]  **What is the best value for the cost function in terms of AUC? How did the number of support vectors change with respect to changes in the cost?**

```{r}

create_cost_models <- function(costs){
  for (i in costs){
    model <- svm(cc ~ ., data = raw.training, type = "C-classification", kernel = "linear", probability = T, cost=i)
    preds <- predict(model, raw.test, probability = TRUE)
    pred_probs <- attr(preds, "probabilities")[,2] 
    roc <- roc(raw.test$cc, pred_probs)
    auc <- auc(roc)
    plot(roc, main=paste("ROC AUC or the lsvm model with cost", toString(i), "AUC: ", auc))
    print(i)
    print(ncol(model$SV))
    print(nrow(model$SV))
    print(paste("AUC:", auc(roc)))
    ci <- ci.auc(raw.test$cc, pred_probs)
    print(paste("95% CI:", ci[1], ",", ci[3]))
  }
}

costs <- c(0.1, 0.5, 1, 5, 10)
create_cost_models(costs)

```

Answer: with lower cost the number of graphs the number of vectors increases a bit. However in terms of the AUC and the number of vectors, numbers above 0.5 seem to provide diminishing returns.

## Que 2.5

Try different kernel functions and assess the discriminative performances comparing the results with those obtained in the previous point. Try a radial, polynomial, and sigmoid kernel in addition to the linear you already tried before.**What is the best classification model on the test set? Optional: Was the second-best model(s) significantly worse than the first one(s) (did the 95% CI ranges of their AUCs overlap)?** Consider also the linear kernel.

```{r}
create_kernel_models <- function(kernels){
  for (i in kernels){
    print(paste("Kernel Model", i))
    model <- svm(cc ~ ., data = raw.training, type = "C-classification", kernel = i, probability = T, cost=1)
    preds <- predict(model, raw.test, probability = TRUE)
    pred_probs <- attr(preds, "probabilities")[,2] 
    roc <- roc(raw.test$cc, pred_probs)
    auc <- auc(roc)
    plot(roc, main=paste("ROC AUC or the lsvm model with kernel", toString(i), "AUC: ", auc))
    print(paste("columns: ", ncol(model$SV)))
    print(paste("rows: ", nrow(model$SV)))
    print(paste("AUC:", auc(roc)))
    ci <- ci.auc(raw.test$cc, pred_probs)
    print(paste("95% CI:", ci[1], ",", ci[3]))
  }
}
kernels <- c("linear", "radial", "polynomial", "sigmoid")
create_kernel_models(kernels)
```

Answer: In terms of AUCs all kernels performed better than the linear one. The sigmoid kernel performed the best with a mean auc of 0.92 and a ci from 0.894 to 0.959 compared to a mean of 0.893 and a ci of 0.856 to 0.929. The CIs overlap so it cannot be said they are significantly better, but the mans are quiet different. 

# 3. Random Forests

Run the RF on the training data. Random forests analysis can be performed by the randomForest function.

```{r}
library(pROC)
library(randomForest)
rf.fit <- randomForest(cc ~ ., data = raw.training, importance = T)
```

## Que 3.1

**What is the output of the model when applied to new data (observations here)? If you want to switch to probabilities/classes, what can you change?** Look at the documentation and also try setting it differently in your code and check what changes.

```{r fig.height=10, fig.width=12}
rf_predictions <- predict(rf.fit, type = "prob")[,2] # type change "prob" to response
# A: the output is a table of predictions with values between 0 and 1 indicating the probability for 
# cc = 0 being true (first column) and cc = 1 being true (second column) we proceed with the second column only 
```

## Que 3.2

Check and think about the type of tree that should be used here and how the type of tree can be used in general.
**Which type of trees are being built (regression or classification)? Which one do you need in this case and why? When do you use one type or the other? How can you set and check the proper type?** Look at the documentation and also try setting it differently and check what changes.

```{r fig.height=10, fig.width=12}
sapply(raw.training$cc, class)
# since cc is a factor, the randomForest function is performing a classification.
# if you cange the cc column to numeric you can perform regression. 
# Since our result is a class of either 1 or 0 classification is appropriate, regression is suitable for continuous values.


```

## Que 3.3

Assess the importance of each predictor and plot them. Are you able to interpret the results? **What are the most important variables? Why?** Plot the relationship between protein value and outcome (cc) for both a protein found to be very important and a protein of low importance

```{r fig.height=30, fig.width=24}
# scaling adjusted to be able to the the varImpPlot
# You can check the importance() and varImPlot() functions
?importance
importance(rf.fit)
varImpPlot(rf.fit,sort= TRUE, n.var=250)

# Most important protein p_141
# least important protein p_9

# Load ggplot2 for plotting
library(ggplot2)

# Plot for highly important protein
ggplot(raw.training, aes(x = cc, y = raw.training$p_141, fill = cc)) +
  geom_boxplot() +
  theme_minimal() +
  labs(title = "Important Protein (141) vs Outcome (cc)",
       x = "Outcome (cc)",
       y = "141")

# Plot for highly important protein
ggplot(raw.training, aes(x = cc, y = raw.training$p_9, fill = cc)) +
  geom_boxplot() +
  theme_minimal() +
  labs(title = "Important Protein (p_9) vs Outcome (cc)",
       x = "Outcome (cc)",
       y = "9")

# Most important variables: 141 66 101 29 38
```

Answer: # Most important variables: 141 66 101 29 38 with a Mean decreased Accuracy of > 10 and a similar Mean DecreaseGini score
# As you can see in the box-plots, a high value for a protein like p_141 changes is indicative of cc = 1, whereas a lower number is usually associated with a cc = 0. In contrast the protein p_9 does not show any significant difference


## Que 3.4

Predict the class of the test set and compute AUC. **What is the AUC? Is this bad, acceptable, good, ...?**

```{r}
# predict on trianing data 
rf_predictions <- predict(rf.fit, newdata = raw.test, type = "prob")[,2]

# create ROC curve
roc_curve <- roc(raw.test$cc, rf_predictions)
auc_value <- auc(roc_curve)

plot(roc_curve, main=paste("ROC Curve random Forest AUC:", toString(auc_value)))
```

Answer: The AUC of ~0.91 is considered high and therefore a good model for the given data.

## Que 3.5

Try to improve the discriminative performances by increasing the number of trees (500-1000-2500) to be grown and assess the AUC. **What is the best number of trees on the test set?**

```{r}

make_n_trees <- function(treecount){
  for (c in treecount) {
    model <- randomForest(cc ~ ., data = raw.training, importance = T, ntree=c)
    # predict on trianing data 
    rf_predictions <- predict(model, newdata = raw.test, type = "prob")[,2]
    
    # create ROC curve
    roc_curve <- roc(raw.test$cc, rf_predictions)
    auc_value <- auc(roc_curve)
    
    plot(roc_curve, main=paste("ROC Curve random Forest n =", toString(c), "AUC:", toString(auc_value)))
    
  }
}

treecounts <- c(50, 100, 500, 1000, 2500)
make_n_trees(treecounts)

# TODO maybe pick out specific sample, adjust the cuttoff value
```

Answer: n=1000 semmes to provide better in terms of AUC score auc ~0.926. n = 500 already provides a solid score with an auc ~0.924. n = 2500 performs worse again. However these difference are only marginally for n > 500 and computing cost / time is increasing significantly. Therefore n = 500 semms to be the sweet spot.

## Que 3.6

**How does the performance of RF compare with the one of SVM? For which cases did SVM/RF struggle the most to make a correct prediction?**

```{r}
# get true negatives of rf model
pred_rf <- predict(rf.fit, newdata = raw.test, type = "response")
confustion_matrix <- table(Predicted = pred_rf, Actual = raw.test$cc)
print(confustion_matrix)

pred_svm <- predict(lsvm.fit, newdata = raw.test, type = "response")
confustion_matrix <- table(Predicted = pred_svm, Actual = raw.test$cc)
print(confustion_matrix)

# In total the sum of false negatives and false positives is 92 (for random forest) and 94 (for svm) indicating that svm is slightly worse. 

```

Answer: The auc of the best svm model is ~ 0.922 and the random forest model ~ 0.926 indicating very good an comparable performance of the two models. 
