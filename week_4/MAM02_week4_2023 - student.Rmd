---
title: "MAM02 assignment week 4"
output: html_notebook 
---

SVM and ensemble methods are elective approaches when data set is characterized by a relatively large feature set and by non-linear decision boundaries. In our case, the dataset corresponds to the simulation of 250 proteins measurements of 1,000 individuals (500 cases [cc = 1] and 500 controls [cc = 0]). Data were split into a training set (70% of the whole data, 700 individuals: 350 cases and 350 controls) and a test set (30% of the whole data, 300 individuals: 150 cases and 150 controls). Of the 250 simulated proteins distributions:

-   some were simulated to be weakly associated to the case/control condition (cc),
-   some were simulated to be strongly associated to the case/control condition,
-   others were simulated to be non-associated associated to the case/control condition with no difference in terms of distribution between cases and controls.

# Libraries

```{r}
install.packages("e1071")
library("pROC")
library("e1071")
library("randomForest")
library("ggplot2")
```

# 1. Import data

```{r}
raw.training <- read.table("simulatedConcentrations.recoded.training.txt", sep = "\t", header = T)
raw.test <- read.table("simulatedConcentrations.recoded.test.txt", sep = "\t", header = T)
raw.training$cc <- as.factor(raw.training$cc)
raw.test$cc <- as.factor(raw.test$cc)
```

## Que 1.1:

**How many rows and columns are in the training and test sets?**

```{r}
ncol(raw.training)
```

Answer: *A: 251

## Que 1.2: 

Compute the correlation matrix for all columns in the training set and plot a histogram of the correlations. Explain in your own words what you can deduce about the data from this. Are there any strong correlations between the variables in the set? Are most correlations positive or negative?

```{r}
# Make a copy of raw.training where the outcome is int instead of factor (needed to include outcome in correlation matrix)
raw.training.outcome_int <- raw.training # copy
raw.training.outcome_int$cc <- as.integer(raw.training.outcome_int$cc) # convert cc to int

# Compute the correlation matrix
# check the cor() function in the help
correlations <- cor(raw.training.outcome_int)
sum(correlations > 0)
sum(correlations <= 0)
# There ae more positive ones overall but by a small margin


# Create histogram via r-base function
# check the hist() function in the help
hist(correlations, breaks = 50)


# Or, create a histogram using ggplot2
# You can use ggplot() + geom_histogram() + theme_linedraw()
# No
```

Answer: There are slighly more positive correalations than negative ones, but this is only marginal. 
The correlations seem to be normally distributed around 0. (TODO can you say that?)

## Que 1.3:

Using the correlation matrix from the previous question, plot a heatmap of the correlations. \
For ease of checking - try to make its colors range from red (-1) to white (0) to blue (1).\
If you are unfamiliar with the heatmap visualization [here](https://www.jmp.com/en_us/statistics-knowledge-portal/exploratory-data-analysis/heatmap.html) is a short explanation

-   Does the heatmap tell you anything further than the histogram?

-   Where would you look at in the heatmap if you wanted to see correlations between the outcome with the other variables?


```{r}
# Plot the heatmap
# check the heatmap() function in the help
#breaks
bk <- c(seq(-1,1,by=0.1))
#colors (one less than breaks
mycols <- c(colorRampPalette(colors = c("red","white", "blue"))(length(bk)-1))
heatmap(correlations,breaks=bk, col=mycols, scale="none")
?heatmap
```

Answer: It shows that most of the correalations are indeed randomly distributed and there are only very few big correlations. 
In thw lower left corner you can see a couple of more correalations, however it is very hard to see which columns are referred to. 
Colors closer to red or blue would indicate higher correlations, so we would look at "bright" spots

## Que 1.4:

**Is it a balanced problem (same number of elements in each class)? Are there any missing values in either train or test sets?**

```{r}
# count elements based on outcome training set
nrow(subset(raw.training, cc == 1))
nrow(subset(raw.training, cc == 0))

nrow(subset(raw.test, cc == 1))
nrow(subset(raw.test, cc == 0))

sum(sapply(raw.training, function(x) sum(is.na(x))) != 0)
sum(sapply(raw.test, function(x) sum(is.na(x))) != 0)
```

Answer: No Missing values. The cc classes are perfectly balanced. 350/350 train-set, 150/150 test-set

# 2. Support vector machines

Fit a linear SVM on the training set data with C classification.

Before we start working on the full 250 variables, just for the fun of if, we created some models with only 2 proteins (to allow visualization in a plot). We selected 2 of the proteins that seem to be more linearly related to the outcome.

First let's plot these 2 proteins to have a visual idea of the dataset, we will use a radial kernel for now...
```{r}

qplot(p_141, p_66, data = raw.training, color = cc )

# Now lets create a model using the different kernels and plot the results to see how the kernels act. We will compare the kernels in a later exercise.

lsvm.fit_only2 <- svm(cc ~ p_141+p_66, data = raw.training, type = "C-classification", kernel = "radial", probability = T)
summary(lsvm.fit_only2)

# performance/AUC
preds = predict(lsvm.fit_only2, newdata=raw.test, probability = TRUE)
pred_probs <- attr(preds, "probabilities")[,2] 
roc <- roc(raw.test$cc, pred_probs)
print(paste("AUC:", auc(roc)))
ci <- ci.auc(raw.test$cc, pred_probs)
print(paste("95% CI:", ci[1], ",", ci[3]))


install.packages("caret")
library(caret)

confusion_table <- table(prediction=preds, actual=raw.test$cc)
confustion_matric <- confusionMatrix(preds, raw.test$cc)                  
accuracy_forFun <- 100*mean(preds==raw.test$cc)
print(paste("The accuracy of the SVM classifier is = ", accuracy_forFun,"%"))

missclassification_forFun <- 100*mean(preds!=raw.test$cc)
print(paste("The missclassifications of the SVM classifier is = ", missclassification_forFun,"%"))
# TODO likely false way and not what they wanted


# Finally lets plot it
plot(lsvm.fit_only2, raw.training, p_141~p_66)

#Lets also take a look at models for the same 2 proteins but using different kernels and see how the AUC is affected
lsvm.fit_only2_linear <- svm(cc ~ p_141+p_66, data = raw.training, type = "C-classification", kernel = "linear", probability = T)
lsvm.fit_only2_poly <- svm(cc ~ p_141+p_66, data = raw.training, type = "C-classification", kernel = "polynomial", probability = T)
lsvm.fit_only2_radial <- svm(cc ~ p_141+p_66, data = raw.training, type = "C-classification", kernel = "radial", probability = T)
lsvm.fit_only2_sigmoid <- svm(cc ~ p_141+p_66, data = raw.training, type = "C-classification", kernel = "sigmoid", probability = T)
plot(lsvm.fit_only2_linear, raw.training, p_141~p_66)
plot(lsvm.fit_only2_poly, raw.training, p_141~p_66)
plot(lsvm.fit_only2_radial, raw.training, p_141~p_66)
plot(lsvm.fit_only2_sigmoid, raw.training, p_141~p_66)

# And their AUC`s
get_auc <- function(model) {
  preds = predict(model, newdata=raw.test, probability = TRUE)
  pred_probs <- attr(preds, "probabilities")[,2] 
  roc <- roc(raw.test$cc, pred_probs)
  plot(roc)
  auc <- auc(roc)
  return(auc)
}
print(get_auc(lsvm.fit_only2_linear))
print(get_auc(lsvm.fit_only2_poly))
print(get_auc(lsvm.fit_only2_radial))
print(get_auc(lsvm.fit_only2_sigmoid))

```
We performed the steps above even though they are not part of the assignment to be able to have a better understanding of the SVM model and of the dataset we are working with (with visualization)...

Now lets stop playing around with only 2 dimensions... back to the assignment, lets fit the SVM using 250 variables (there are ways to visualize 5 dimensions in a plot (using color and size for example), or 6 dimensions (adding frequency of vibration) if animation is used... for 250 dimensions it is impractical)
```{r}
lsvm.fit <- svm(cc ~ ., data = raw.training, type = "C-classification", kernel = "linear", probability = T)
```

## Que 2.1

Check the `svm` API in the help to help you answer this. **What is the meaning of setting `probability = T`? What does change if you set it to false? And if you remove it, what is the default? ** Look at the documentation and also try setting it differently in your code and check what changes.

```{r}
?smv
```

Answer: The setting probability = T allows for probability predictions

## Que 2.2

Inspect the resulting support vectors (number and type). Check the `svm` API in the help and think how you can achieve this. **What is the percentage of training data used for classification? Which support vector has the largest magnitude? Which has the smallest?**

```{r}
View(lsvm.fit$SV)
sapply(lsvm.fit$SV, class)
nrow(lsvm.fit$SV)
ncol(lsvm.fit$SV)
```

Answer: there are 250 support Vecors with a length of 216. All vectors are numeric. 
TODO  ?

## Que 2.3

Predict the class of the test set and estimate the discriminative performances by computing the AUC. **What is the AUC? Is this bad, acceptable, good, ...? Optional: What is the 95% CI of the AUC? what does it mean?**

```{r}
preds <- predict(lsvm.fit, raw.test, probability = TRUE)
pred_probs <- attr(preds, "probabilities")[,2] 
roc <- roc(raw.test$cc, pred_probs)
plot(roc, main="ROC AUC or the lsvm model")
print(paste("AUC:", auc(roc)))
ci <- ci.auc(raw.test$cc, pred_probs)
print(paste("95% CI:", ci[1], ",", ci[3]))

```

Answer: The ROC is pretty high for the dataset with a AUC or ~0.89 and a confidence interval form ~0.86 to ~0.93. This means that the true AUC is between 0.86 and 0.93. Both are acceptably high values

## Que 2.4

Repeat the exercise by varying the value of the parameter "cost" and generate a table of results with the value of cost and the corresponding number of support vectors, and AUC on the test data. Use at least these 3 costs: [0.1, 1, 10]  **What is the best value for the cost function in terms of AUC? How did the number of support vectors change with respect to changes in the cost?**

```{r}

create_cost_models <- function(costs){
  for (i in costs){
    model <- svm(cc ~ ., data = raw.training, type = "C-classification", kernel = "linear", probability = T, cost=i)
    preds <- predict(model, raw.test, probability = TRUE)
    pred_probs <- attr(preds, "probabilities")[,2] 
    roc <- roc(raw.test$cc, pred_probs)
    auc <- auc(roc)
    plot(roc, main=paste("ROC AUC or the lsvm model with cost", toString(i), "AUC: ", auc))
    print(i)
    print(ncol(model$SV))
    print(nrow(model$SV))
    print(paste("AUC:", auc(roc)))
    ci <- ci.auc(raw.test$cc, pred_probs)
    print(paste("95% CI:", ci[1], ",", ci[3]))
  }
}

costs <- c(0.1, 0.5, 1, 5, 10)
create_cost_models(costs)

```

Answer: with lower cost the number of graphs the number of vectors increases a bit. However in terms of the AUC and the number of vectors, numbers above 0.5 seem to provide diminishing returns.

## Que 2.5

Try different kernel functions and assess the discriminative performances comparing the results with those obtained in the previous point. Try a radial, polynomial, and sigmoid kernel in addition to the linear you already tried before.**What is the best classification model on the test set? Optional: Was the second-best model(s) significantly worse than the first one(s) (did the 95% CI ranges of their AUCs overlap)?** Consider also the linear kernel.

```{r}
create_kernel_models <- function(kernels){
  for (i in kernels){
    print(paste("Kernel Model", i))
    model <- svm(cc ~ ., data = raw.training, type = "C-classification", kernel = i, probability = T, cost=1)
    preds <- predict(model, raw.test, probability = TRUE)
    pred_probs <- attr(preds, "probabilities")[,2] 
    roc <- roc(raw.test$cc, pred_probs)
    auc <- auc(roc)
    plot(roc, main=paste("ROC AUC or the lsvm model with kernel", toString(i), "AUC: ", auc))
    print(paste("columns: ", ncol(model$SV)))
    print(paste("rows: ", nrow(model$SV)))
    print(paste("AUC:", auc(roc)))
    ci <- ci.auc(raw.test$cc, pred_probs)
    print(paste("95% CI:", ci[1], ",", ci[3]))
  }
}
kernels <- c("linear", "radial", "polynomial", "sigmoid")
create_kernel_models(kernels)
```

Answer: In terms of AUCs all kernels performed better than the linear one. The sigmoid kernel performed the best with a mean auc of 0.92 and a ci from 0.894 to 0.959 compared to a mean of 0.893 and a ci of 0.856 to 0.929. The CIs overlap so it cannot be said they are significantly better, but the mans are quiet different. 

# 3. Random Forests

Run the RF on the training data. Random forests analysis can be performed by the randomForest function.

```{r}
rf.fit <- randomForest(cc ~ ., data = raw.training, importance = T)
```

## Que 3.1

**What is the output of the model when applied to new data (observations here)? If you want to switch to probabilities/classes, what can you change?** Look at the documentation and also try setting it differently in your code and check what changes.

```{r fig.height=10, fig.width=12}
<complete code>
```

## Que 3.2

Check and think about the type of tree that should be used here and how the type of tree can be used in general.
**Which type of trees are being built (regression or classification)? Which one do you need in this case and why? When do you use one type or the other? How can you set and check the proper type?** Look at the documentation and also try setting it differently and check what changes.

```{r fig.height=10, fig.width=12}
<complete code>
```

## Que 3.3

Assess the importance of each predictor and plot them. Are you able to interpret the results? **What are the most important variables? Why?** Plot the relationship between protein value and outcome (cc) for both a protein found to be very important and a protein of low importance

```{r fig.height=10, fig.width=12}
# You can check the importance() and varImPlot() functions
<complete code>
```

Answer: **include answer**

## Que 3.4

Predict the class of the test set and compute AUC. **What is the AUC? Is this bad, acceptable, good, ...?**

```{r}
<complete code>
```

Answer: **include answer**

## Que 3.5

Try to improve the discriminative performances by increasing the number of trees (500-1000-2500) to be grown and assess the AUC. **What is the best number of trees on the test set?**

```{r}
<complete code>
```

Answer: **include answer**

## Que 3.6

**How does the performance of RF compare with the one of SVM? For which cases did SVM/RF struggle the most to make a correct prediction?**

```{md}
<complete code>
```

Answer: **include answer**
